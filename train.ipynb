{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from time import time\n",
    "\n",
    "from dataloader import ImageDataset\n",
    "from model import *\n",
    "from utils import save_model\n",
    "\n",
    "np.random.seed(69)\n",
    "torch.manual_seed(69)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(f'Using: {device}')\n",
    "if str(device) == 'cuda': print(torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 200\n",
    "IMAGE_SIZE = 512\n",
    "\n",
    "# for checkpoints\n",
    "SAVE_CHECKPOINTS = False\n",
    "CHECKPOINT_INTERVAL = 5\n",
    "\n",
    "# for preload\n",
    "preload = True\n",
    "preload_optimizer = False\n",
    "checkpoint_model = 'SRTransformer6_best_86.pth'\n",
    "\n",
    "train_set = ImageDataset(\"data/train/\", 2, size=IMAGE_SIZE)\n",
    "test_set = ImageDataset(\"data/validation/\", 2, size=IMAGE_SIZE)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# init model\n",
    "model = SRTransformer6()\n",
    "model.to(device)\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "loss_mse = nn.MSELoss()\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=3.56E-04)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder\n",
    "from utils import save_plot\n",
    "\n",
    "# search for the best learning rate\n",
    "lr_finder = LRFinder(model, optimizer, loss_fn, device=device)\n",
    "lr_finder.range_test(train_loader, start_lr=1e-6, end_lr=1, num_iter=50)\n",
    "ax, lr = lr_finder.plot()\n",
    "lr_finder.reset()\n",
    "\n",
    "# save figure\n",
    "fig = ax.get_figure()\n",
    "save_plot(fig, f'{model.__class__.__name__}_lr_finder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other model\n",
    "if preload:\n",
    "    checkpoint = torch.load('models/'+checkpoint_model)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch = checkpoint['epoch'] + 1\n",
    "    if preload_optimizer: \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "else:\n",
    "    epoch = 0\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "best_loss = None\n",
    "start_epoch = epoch\n",
    "for epoch in range(epoch, EPOCHS):\n",
    "    t0 = time()\n",
    "    all_losses.append([])\n",
    "    model.train() # training mode\n",
    "    for batch in train_loader:\n",
    "        # load data to the device\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "\n",
    "        out = model.forward(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # hold the loss\n",
    "        all_losses[-1].append(loss.item())\n",
    "\n",
    "    model.eval() # evaluation mode\n",
    "    with torch.no_grad():\n",
    "        t1 = time()\n",
    "        test_loss = 0\n",
    "        test_loss_mse = 0\n",
    "        for batch in test_loader:\n",
    "            # load data to the device\n",
    "            x, y = batch[0].to(device), batch[1].to(device)\n",
    "            out = model.forward(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            loss_mse_data = loss_mse(out, y)\n",
    "            test_loss += loss.item()\n",
    "            test_loss_mse += loss_mse_data.item()\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        print(f'{epoch}: Val loss (MSE): {test_loss_mse:.6f} | Val loss: {test_losses[-1]:.6f} | loss: {sum(all_losses[-1])/len(train_loader):.6f} | Train time: {t1-t0:.2f} | Test time: {time()-t1:.2f}')\n",
    "\n",
    "    if SAVE_CHECKPOINTS and epoch % CHECKPOINT_INTERVAL == CHECKPOINT_INTERVAL-1:\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }\n",
    "        save_model(state, 'models', f'{model.__class__.__name__}_{epoch:02d}.pth')\n",
    "    \n",
    "    # save the best model\n",
    "    if best_loss is None or best_loss > test_losses[-1]:\n",
    "        best_loss = test_losses[-1]\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }\n",
    "        save_model(state, 'models', f'{model.__class__.__name__}_best_{epoch:02d}.pth')\n",
    "\n",
    "state = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}\n",
    "save_model(state, 'models', f'{model.__class__.__name__}_{epoch:02d}_final.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils import save_plot\n",
    "\n",
    "train_losses = [sum(l)/len(train_loader) for l in all_losses]\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "epoch_data = list(range(start_epoch, start_epoch+len(train_losses)))\n",
    "plt.plot(epoch_data, test_losses, label='validation')\n",
    "plt.plot(epoch_data, train_losses, label='train')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "save_plot(fig, f'{model.__class__.__name__}_losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
